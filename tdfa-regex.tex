\documentclass[english]{sigplanconf}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage[english]{babel}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{refstyle}
\usepackage{float}
\usepackage{textcomp}
\usepackage{amsthm}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[lined,linesnumbered,commentsnumbered]{algorithm2e}
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\newtheorem*{notation}{Notation}

\lstdefinestyle{sharpc}{language=mathescape, frame=lr, rulecolor=\color{blue!80!black}}

% ============================================================
%% Uncomment the next few lines to get sf url links:
%\usepackage{url}            
%\makeatletter
%\def\url@leostyle{%
%  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\sffamily}}}
%\makeatother
%\urlstyle{leo} % Now actually use the newly defined style.
%% Choose coloured or b/w links:
\usepackage[pdftex,colorlinks=true,pdfstartview=FitV,
 linkcolor=black,citecolor=black,urlcolor=black]{hyperref}
% \usepackage{hyperref}
\usepackage{needspace}
\newcommand{\needlines}[1]{\Needspace{#1\baselineskip}}
% ============================================================
% Markup macros for proof-reading
\usepackage{ifthen}
\usepackage[normalem]{ulem} % for \sout
\usepackage{xcolor}
\newcommand{\ra}{$\rightarrow$}
\newboolean{showedits}
\setboolean{showedits}{true} % toggle to show or hide edits
\ifthenelse{\boolean{showedits}}
{
	\newcommand{\ugh}[1]{\textcolor{red}{\uwave{#1}}} % please rephrase
	\newcommand{\ins}[1]{\textcolor{blue}{\uline{#1}}} % please insert
	\newcommand{\del}[1]{\textcolor{red}{\sout{#1}}} % please delete
	\newcommand{\chg}[2]{\textcolor{red}{\sout{#1}}{\ra}\textcolor{blue}{\uline{#2}}} % please change
}{
	\newcommand{\ugh}[1]{#1} % please rephrase
	\newcommand{\ins}[1]{#1} % please insert
	\newcommand{\del}[1]{} % please delete
	\newcommand{\chg}[2]{#2}
}
% ============================================================
% Put edit comments in a really ugly standout display
%\usepackage{ifthen}
\usepackage{amssymb}
\newboolean{showcomments}
\setboolean{showcomments}{true}
%\setboolean{showcomments}{false}
\newcommand{\id}[1]{$-$Id: scgPaper.tex 32478 2010-04-29 09:11:32Z oscar $-$}
\newcommand{\yellowbox}[1]{\fcolorbox{gray}{yellow}{\bfseries\sffamily\scriptsize#1}}
\newcommand{\triangles}[1]{{\sf\small$\blacktriangleright$\textit{#1}$\blacktriangleleft$}}
\ifthenelse{\boolean{showcomments}}
%{\newcommand{\nb}[2]{{\yellowbox{#1}\triangles{#2}}}
{\newcommand{\nbc}[3]{
 {\colorbox{#3}{\bfseries\sffamily\scriptsize\textcolor{white}{#1}}}
 {\textcolor{#3}{\sf\small$\blacktriangleright$\textit{#2}$\blacktriangleleft$}}}
 \newcommand{\version}{\emph{\scriptsize\id}}}
{\newcommand{\nbc}[3]{}
 \renewcommand{\ugh}[1]{#1} % please rephrase
 \renewcommand{\ins}[1]{#1} % please insert
 \renewcommand{\del}[1]{} % please delete
 \renewcommand{\chg}[2]{#2} % please change
 \newcommand{\version}{}}
\newcommand{\nb}[2]{\nbc{#1}{#2}{orange}}
\newcommand{\here}{\yellowbox{$\Rightarrow$ CONTINUE HERE $\Leftarrow$}}
\newcommand\rev[2]{\nb{TODO (rev #1)}{#2}} % reviewer comments
\newcommand\fix[1]{\nb{FIX}{#1}}
\newcommand\todo[1]{\nb{TO DO}{#1}}
\newcommand\on[1]{\nbc{ON}{#1}{red}} % add more author macros here
%\newcommand\XXX[1]{\nbc{XXX}{#1}{blue}}
%\newcommand\XXX[1]{\nbc{XXX}{#1}{brown}}
%\newcommand\XXX[1]{\nbc{XXX}{#1}{cyan}}
%\newcommand\XXX[1]{\nbc{XXX}{#1}{darkgray}}
%\newcommand\XXX[1]{\nbc{XXX}{#1}{gray}}
%\newcommand\XXX[1]{\nbc{XXX}{#1}{magenta}}
%\newcommand\XXX[1]{\nbc{XXX}{#1}{olive}}
%\newcommand\XXX[1]{\nbc{XXX}{#1}{orange}}
%\newcommand\XXX[1]{\nbc{XXX}{#1}{purple}}
%\newcommand\XXX[1]{\nbc{XXX}{#1}{red}}
%\newcommand\XXX[1]{\nbc{XXX}{#1}{teal}}
%\newcommand\XXX[1]{\nbc{XXX}{#1}{violet}}
% ============================================================


\begin{document}
\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{CONF 'yy}{Month d--d, 20yy, City, ST, Country}
\copyrightyear{20yy} 
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
\doi{nnnnnnn.nnnnnnn}

\frenchspacing

\title{Efficient regular expressions that produce parse trees}

\authorinfo{John}

\authorinfo{John}
      
\authorinfo{John}

\maketitle

\begin{abstract}
Regular expressions naturally and intuitively define ASTs that describe
the text that they're parsing.
\on{But existing approaches fail to keep track of all regex matches within the AST.}
We describe a technique for building
up the complete parse tree resulting from matching a text against
a regular expression.

In standard TDFA matching, all paths through the NFA are walked simultaneously,
as if in different threads, where inside each thread, it is fully
known when which capture group was entered or left. We extend this
model to keep track of not just the last opening and closing of capture
groups, but all of them. We do this by storing, in every thread, using
the fly-weight pattern, a history of the all groups. Thus, we log
enough information during parsing to build up the complete AST at
the end of parsing.
\end{abstract}


\section{Introduction}

\global\long\def\regex{\text{\text{regular expression}}}
A regular expression can easily describe that a text matches a comma
separated values file, but it is unable to extract all the values.
Instead it will only give a single instance of values: \texttt{((.*?),(\textbackslash d+);)+} might describe a dataset of ASCII names
with their numeric label. Matching the regular expression on \texttt{``Tom
Lehrer,1;Alan Turing,2;''} will confirm that the list is well formed,
but the match will only contain \texttt{``Tom Lehrer''} for the
second capture group and \texttt{``1''} for the third. That is,
the parse tree found by the \textsc{posix} is: 

\begin{figure}[h]
\centering
\includegraphics[width=.75\linewidth]{graphs/posix_parse}
\caption{\figlabel{lehrer-posix} Parse tree produced by \textsc{posix}-compatible matching \texttt{((.*?),(\textbackslash d+);)+} against input ``Tom
Lehrer,1;Alan Turing,2;''.}
\end{figure}

With our algorithm we are able to reconstruct the full parse
tree after the matching phase is done:

\begin{figure}[h]
\centering
\includegraphics[width=.75\linewidth]{graphs/full_parse}
\caption{\figlabel{lehrer-tree} Parse tree produced by our approach matching regular expression \texttt{((.*?),(\textbackslash d+);)+} against input ``Tom
Lehrer,1;Alan Turing,2;''}
\end{figure}

The amortized run time of our approach is $O(m\: n)$, where $m$
is the length of the regular expression and $n$ is the length of
the parsed string. This is asymptotically as fast as the best matchers
that don't extract parse trees. 

\subsection{More powerful than standard regular expressions}
\seclabel{power}
It may at first seem as if all capture groups can always, equivalently,
be extracted by splitting the input, and then applying sub-regular
expressions on the splits.  This is, for example, an entirely valid
strategy to extract the parse tree in \Figref{lehrer-tree}.  However,
this can quickly become an exercise of writing an entire parser,
using no regular expression engine at all, even if the underlying
grammar is entirely regular.  The following grammar is hard to parse
using a regular expression engine, even though it is regular.


Consider a file of semicolon-terminated records, each record consists
of a comma-separated pair of entries, and each entry can be escaped
to contain semicolons, as in the regular expression
\texttt{((".*?"|[a-z]*),(".*?"|[a-z]*);)+}. Here, expression
\texttt{.*?} is a non-greedy match which will be discussed in more
detail in \Secref{algo}.  This language contains, for example, the
string: ``"h;i",there;"h;,i",Paul;''.  It is easy to see that, in
order to extract all four capture group matches, it is insufficient
to split the input at the semicolon, as that would split the field
``h;i'' in half.  More involved examples, where hand-written parsers
become harder to make, are easily constructed.  In contrast, our
approach yields the entire parse tree, simply from specifying the
above regular expression.


\subsection{Motivation}

This is the age of big data, and the first step of processing big
data is often to parse strings. As an example, consider log files.
What makes data huge is typically repetition. As Jacobs\cite{Jaco09a}
noted, ``What makes most big data big is repeated observations over
time and/ or space,'' and thus log files grow large frequently. At
the same time, they provide important insight into the process that
they are logging, so their parsing and understanding is important. 

Regular expressions make for scalable and efficient lightweight parsers.\cite{Kart96a} 

The parsing abilities of regular expression have evoked Meiners to declare
that for intrusion detection, ``fast and scalable RE matching is
now a core network security issue.'' \cite{Mein10a}

For example, Arasu et al. \cite{Aras12a} demonstrate how regular
expressions are used in Bing to validate data, by checking whether
the names of digital cameras in their database are valid.

Parsers that can return abstract syntax trees are more useful than
ones that only give a flat list of matches. Of course only regular
grammars can be matched by our approach.

\ml{This next paragraph falls out of the sky a bit :)}
Let us recall what NFAs and DFAs are. A DFA is a state machine that
will walk over the transition graph, one step for every input
character. The choice of transition is limited by the transition's
character range. A transition can only be followed if the current
input character is inside transition's character range. NFAs differ
from DFAs in that for some input character and some state, there may
be more than \ins{one} applicable transition. If there is, an NFA will magically
guess the correct one. \autoref{fig:example-automaton} shows an example of
an NFA's transition graph. For the moment, let us discuss regular
expression matching on NFAs. Assuming that the NFA just magically knows
the right transition lets us focus on the important things, greediness control
and capture groups.

\section{Algorithm}
\seclabel{algo}
Conceptually, our approach is the following pipeline of four stages.
\begin{enumerate}
  \item Parse the regular expression string into an AST.
  \item Transform the AST to an NFA.
  \item Transform the NFA to a DFA.
  \item Compactify the DFA.
\end{enumerate}

In reality, things are a little more involved, since the transformation
to DFA is lazy, and the compactification only happens after no lazy
compilation occurred in a while. Worse, compactification can be
undone if needed. We'll get back to these details in \secref{implementation}.
Let's discuss the stages in turn, starting with 2, since step 1,
the parsing of the regular expression grammar, is reasonably
straightforward.


\subsection{Thompson's construction} 

We transform the AST of the regular expression into an NFA,
in a modified version of Thompson's NFA construction. To
control greedines, or discern capture groups, our approach adds
$\varepsilon$ transitions to the transition graph. An
$\varepsilon$-transition has no input range assigned, and can thus always
be used. It does not consume an input character.
The additions are needed for greediness control and capture groups.
Let's look at both, in turn.

To see the importance of greediness control, consider again the regular
expression \texttt{((.*?),(\textbackslash{}d+);)+}. The question
mark sets the \texttt{.*} part of the regular expression to
\emph{non-greedy}, which means that it will match as little as
possible while still producing a valid match, if any.  Without
provisioning \texttt{.*} to be non-greedy, a matching against input
\texttt{``Tom Lehrer,1;Alan Turing,2;''} would match as much as
possible into the first capture group, including the record separator
`,'.  Thus, the first capture group would suddenly contain only one
entry, and it would contain more than just names, namely``Tom
Lehrer,1;Alan Turing''.  This is, of course, not what we expect.
Non-greediness, here, ensures that we get ``Tom Lehrer'', then
``Alan Turing'' as the matches of the first capture group.

In the NFA, we model greedy repetition or non-greedy repetition of
an expression as follows.  First, we construct an NFA graph for the
expression, without any repetition.  We can see how this plays out
in our running example, which contains the expression \texttt{.*?}.
First, an automaton for \texttt{.} is constructed.  As we can see
in \autoref{fig:example-automaton}, expression \texttt{.} is modeled as
just two nodes labeled 3 and 4, and a transition labeled ``any''
between them.  Repeating is achieved by adding two $\varepsilon$
transitions: one from 4 back to 3, to match more than one time any
character, and another one from 3 to 4, to enable matching nothing
at all.  Importantly, the transition from 4 back to 3 is marked as
low priority, while the transition leaving the automaton, from 4
to 5, is unmarked, which means normal priority.  This means that
the NFA will prefer leaving the repeating expression, rather than
staying in it.  If the expression were greedy, then we would mark
the transition from 4 to 5 as low-priority, and the NFA would prefer
to match any character repeatedly.

More generally, the NFA will prefer to follow transitions of normal
priority over those of low priority. Rather than formalize this
notion of preference on NFAs, we come back to prioritized edges when
discussing the transformation from NFA states to DFA states.


\begin{figure*}[tb]
\includegraphics[width=\linewidth]{graphs/thompson}
\caption{Modified Thompson~\cite{Thom68a} construction of the automaton: Decent into the abstract syntax tree of the regular expression and expand the constructs recursively.}
\label{fig:thompson-construction}
\end{figure*}

To model capture groups in the NFA, we add \emph{commit tags} to the
transition graph. The transition into a capture group is tagged by a
commit, the transition to leave a capture group is tagged by a another
commit. We distinguish opening and closing commits. The NFA keeps a
history for every transition with a commit. Each time the transition
is walked, the current position is added to its history. 

We model histories as a linked list, where the payload of each node
is a position.  Only the payload of the \emph{head}, the first node,
is mutable, the \emph{rest}, all other nodes, are immutable.  Because
the rest are immutable, they may be shared between histories.  This
is an application of the flyweight pattern, which ensures that all
of the following instructions on histories can be performed in
constant time. Here, the \emph{position} is the current position
of the matcher.  It is well-defined at all times since all threads
are executed in lock-step, such that at any point in time, all
threads are at the same position.

\begin{notation}
We denote NFA states with $q_i$ and their array of histories 
$(h_1, \dots, h_n)$. Since a history is actually a
list of positions in the matched string, we will write it
as $h_i=[x_1, \dots, x_n]$ to describe that past matches occurred
at the positions $x_1, \dots, x_n$. 

Each NFA state has an array of $2n$ histories, where 
$n$ is the number of capture groups in the original 
regular expression. This is written as 
$(h_1, h_2, \dots h_{2n-1}, h_{2n})$. At position $2i$ 
is the history of the opening positions of
the capture group $i$, and at position $2i+1$ is the closing history.

Take for example the regular expression \texttt{(..)+} matching 
pairs of characters on the string ``abcd'', then the history array describing this would be
$[h_1=[2,0], h_2=[3,1]]$.
\end{notation}

\global\long\def\naturals{\mathbb{N}}
\global\long\def\integers{\mathbb{Z}}
\global\long\def\pos{\mathbf{\mathbf{p}}}

\begin{figure}
\includegraphics[width=\linewidth]{graphs/lehrer_automaton}

\caption{\label{fig:example-automaton}
Automaton for \texttt{((.*?),(\textbackslash{}d+);)+} 
In the diagram, ``$-$'' stands for low priority. $\tau_n\uparrow$ is the opening tag for capture group $n$, likewise, $\tau_1\downarrow$ is the closing tag for capture group $n$.}
\end{figure}

\subsection{DFAs}

Our above definition of regular expression assumes a machine that
guesses the correct transition through magic. To implement regular
expression matching without supernatural intervention, we lazily transform
the NFA to a DFA. 

A useful metaphor for regular expression matching is that of threads
\cite{Cox07a}. Whenever we aren't sure which transition to take,
we ``fork'' a thread for every option that we have. This way, when
the input is over, there must be at least one thread that guessed
correctly at all times. We use the word ``thread'' here to guide
intuition only. Our approach is not parallel. 

The key insight is that we keep all ``threads'' in lock-step. To
achieve this, we must be very specific about what constitutes the
state of a thread. Since every thread effectively simulates a different
NFA, the state inside of a thread contains exactly two items: the
NFA state it simulates and the history for every tag. Now, the following
would be a correct, although slow, implementation of a non-magic NFA
interpreter: whenever an input character is read, we can iterate over
all threads, kill the ones that have no legal transition for the input
character, and fork more threads as needed.

Trouble starts when we want to fork a thread for an NFA state that
is already running. Not only is an explosion of threads bad for
performance, it would also lead to ambiguity: if the two threads
disagree on the histories, which one is correct?

Algorithm~\ref{onestep}  takes as an input a set of threads, an NFA
transition graph, and an input character, and returns the set of threads
running after the input character has been read. It makes sure that
if there could be two threads with the same NFA state, the one that
follows greedy matching\footnote{Non-greedy operators work the same way, just
with reversed priorities} will survive. At no point of the algorithm are the
two states both present in the result and thus in conflict. This is avoided
by prioritizing the edge with the result that follows the greedy match and
not considering states visited before, ensuring the greedy thread 
``wins the race''.

\begin{notation} In the following examples, we will use the following:

\begin{itemize}
	\item	Each DFA state is denoted by a capital letter, e.g. $Q$, and contains multiple NFA 
			threads with their current histories in the order in which they are scheduled. 
			\[Q=[(q_1, (h_1, h_2, h_3, h_4, h_5, h_6)), (q_2, (h_1, h_2, h_3, h_4, h_7, h_8))]\] for
			example means that the current DFA state has one thread in $q_1$ with 
			histories $(h_1, h_2, h_3, h_4, h_5, h_6)$ and another in $q_2$ with the histories
			$(h_1, h_2, h_3, h_4, h_7, h_8)$. Note that histories can be shared across threads
			if they have the same matches.
	\item	A transition is understood to be between NFA states, $q_1\rightarrow q_2$
			means a transition from $q_1$ to $q_2$.
			The meta-data about the 
			transition is described in the text and can contain consumption
			of characters, priority or opening and closing commit tags.
	\item	The algorithm uses two stacks, called \emph{high} and \emph{low}. The algorithm uses these two stacks to handle the scheduling
	of the threads. The LIFO\footnote{last-in-first-out} property ensures that threads preferably are run until they can't follow further $\varepsilon$-edges. The \emph{high} stack takes strict priority
	over the \emph{low} stack.
			\on{NB: don't use math mode for words, use emph, or the spacing will be wrong. Within math mode use the textit macro.}
	\item	In the examples we will pretend for clarity that the commands
			described below are executed directly after they
			are encountered. The actual algorithm collects them and executes 
			them after the \emph{oneStep} call to allow further optimizations.
\end{itemize}

\begin{description}
\item [$h\leftarrow\pos$] Stores the current position into the head of the linked list $h$.
\item [$h\leftarrow\pos+1$] Stores the position after the current one into the head of linked list $h$.
\item [$h'\mapsto h$] Sets head.next of $h$ to be head.next of $h'$. 
	This effectively copies the (immutable) rest of $h$ to be the rest of $h'$, also. 
\item [$c\uparrow(h)$ and $c\downarrow(h)$] Prepends linked list $h$ with a new node, which becomes the new head.
This effectively \emph{commits} the old head, which is henceforth considered immutable. $c\uparrow(h)$ 
describes the opening position of the capture group and is therefore called the opening commit. 
Conversely $c\downarrow(h)$ is the closing commit that marks the end of the capture group. 
This distinction is only for clarity, the semantic is the same for opening and closing commits.
\end{description}
\end{notation}

\begin{algorithm*}[htb]
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\DontPrintSemicolon
\SetAlgoLined
\Input{$input$ is a sequence of characters}
\Output{a tree of matching capture groups for the regex}
\Begin{
\tcp{Lazily compiles a DFA while matching.}

Set $Q$ to startState.\;
\tcp{A thread is an NFA state, with an array of histories.}
Let $Q$ be all threads that are reachable in the NFA transition graph by following $\varepsilon$ transitions only.\;
Execute instructions described in $oneStep$ when walking $\varepsilon$ transitions.\;

\tcp{Create the transition map of the DFA.}
Set $T$ to an empty map from state and input to new state and instructions.
\;
\tcp{Consume string}
\ForEach{position $pos$ in $input$}{
	Let $a$ be the character at position $pos$ in $input$.
	
	\eIf{$T$ has an entry for $Q$ and $a$}{
		\tcp{Let the DFA handle $a$}
		Read the instructions and new state $Q'$ out of $T$\;
		execute the instructions\;
		$Q\leftarrow Q'$\;
		jump back to start of for loop.\;
	}{ %else
		\tcp{lazily compile another DFA state.}
		Run $oneStep(Q,a)$ to find new state $Q'$ and instructions \;
		Run $findMapping(Q', T)$ to see if Q' can be mapped to an existing state $Q''$\;
		\eIf{$Q''$ was found}{
			Append the mapping instructions from $findMapping$ to the instructions found by $oneStep$\;
			Execute the instructions.\;
			Add an entry to $T$, from current state $Q$ and $a$, to new state $Q''$ and instructions.\;
			Set $Q$ to $Q''$\;
		}{ %else
			Execute the instructions found by oneStep.\;
			Add an entry to $T$, from current state $Q$ and $a$, to new state $Q'$ and instructions.\;
			Set $Q$ to $Q'$.\;
		}
	}

}
}
\caption{$interpret(input)$: Interpretation and lazy compilation of the NFA}
\end{algorithm*}

\begin{algorithm*}[htb]
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\DontPrintSemicolon
\SetAlgoLined
\Input{Graph of transitions for an NFA,\\
	   Input character $a$,\\
	   Input position $pos$,\\
	   a list of threads $Q = [(q, [h_{1},\dots,h_{n}])]$,
	     where $q$ is an NFA and $[h_{1},\dots,h_{n}]$ is an array of histories.}
\Output{ Set of threads $R$.}
\Begin{

$R\leftarrow []$\;

Initialize empty stack $buffer$\;
Initialize empty stack $high$\;
Initialize stack $low$ so that $(q, h)\in Q$ are retrieved front to back of $Q$.\;
Mark all threads in \emph{low} as hungry.

\tcp{Follow transitions greedily}
\While{$high$ and $low$ are not both empty}{
	\eIf{$high$ not empty}{
		pop $(q', [h_{1},\dots,h_{n}])$ from $high$\;
	}{
		pop $(q', [h_{1},\dots,h_{n}])$ from $low$\;
		
		\lWhile{$buffer$ is not empty}{ 
			pop $(q, [h_{1},\dots,h_{n}])$ from $buffer$ and add it to $R$\;
		}
	}
	\If{current thread is marked hungry}{
		\ForEach{$a$-consuming transition $e=q'\rightarrow q''$}{
			push $(q'', [h_{1},\dots,h_{n}])$ to $high$.\;
		}
		jump to top of while.\;
	}
	\lIf{$q'$ is marked as seen}{ jump to top of while loop \;}
	mark $q'$ as seen\;
	add $(q', [h_{1},\dots,h_{n}])$ to $buffer$\;
	\ForEach{$\varepsilon$-transitions $t$ from $q'$ to $q''$}{
		\lIf{$q''$ is marked as seen}{continue for loop\;}
		\If{$t$ is tagged with an open or close tag}{
			Choose $i$ such that $h_{i}$ is the history of  $t$'s open tag\;
			Make a new history $h'$\;
			$h_{i} \mapsto h'$ \label{algline:old-copy}\tcp*{Copy old history}
			\eIf{$t$ has a open tag}{
				Let $newHistories$ be $[\dots,h_{i-1},h',h_{i+1},\dots]$\;
				$h' \leftarrow pos+1$ \label{algline:store-after}\tcp*{Store position after current}
			}{
				\tcp{$t$ has a close tag}
				Choose $i'$ such that $h_{i'}$ is the history of  $t$'s open tag \;
				Make a new history $h''$\;
				$h'_{i'} \mapsto h''$ \label{algline:old-copy2}\tcp*{Copy old history}
				$h'' \leftarrow pos$\label{algline:store-current}\tcp*{Store current position}
				Commit $h'$\;
				Commit $h''$\;
				Let $newHistories$ be $[\dots,h_{i-1},h', \dots, h'',h_{i'+1},\dots]$\;
			}
		}					
				
		\tcp{Push according to priority of transition:}
		\eIf{$t$ has low priority}{
			push $(q'', newHistories)$ to $low$\;
		}{
			push $(q'', newHistories)$ to $high$\;
		}
	}
}
}

\caption{\label{onestep}$oneStep(NFA, a, pos, Q)$: Compute the follow-up state for DFA state $Q$}
\end{algorithm*}

\clearpage

\begin{algorithm*}[htb]
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\DontPrintSemicolon
\SetAlgoLined
\Input{$Q=\{(q_i, h_i)\}_{i=1\dots n}$ is a DFA state.}
\Output{A state $Q'$ that $Q$ is \emph{mappable} to.\\
	The ordered instructions $m$ that reorder the memory 
		locations of $Q$ to $Q'$ and don't interfere with each other.}
\Begin{
    
\ForEach{ $Q'$ such that $\{q_i\} = \{q'_i\}$}{

	\tcc{Invariant: For each history $H$ there is at most one $H'$\\ so that $H\leftarrow H'$ is part of the mapping.}
	Initialize empty map $m$\;
	
	\ForEach{$q_i=q'_i$ with histories $H$ and $H'$ respectively}{
		\For{$i=0\dots length(H)-1$}{
    
			
			\eIf{$H(i)$ is in $m$ as a key already and does not map to $H'(i)$}{
				Fail\;
			}{
				 \tcc{Hypothesize that this is part of a valid map}
				Add $H(i) \mapsto H'(i)$ to $m$;
				}
			}
		}
    }
	\tcp{The mapping was found and is in $m$ if we didn't fail.}
  
	sort $m$ in reverse topological order so that no values are overwritten. \;
  
	\Return{$Q'$ and $m$}
}
\caption{\label{findmapping}$findMapping(Q)$: Finding a state that $Q$ is mappable to in order to keep the number of states created bound by the length of the regular expression.}
\end{algorithm*}

\begin{example} Execution of algorithm \ref{onestep}:
\label{ex:oneStep1}

Assume as an example the automaton in figure \ref{fig:example-automaton} is in the DFA state\footnote{NFA states that do not have any outgoing transitions except for $\varepsilon$ transitions can be omitted, since the first step will only consider edges which consume the next character of the input.} 
\begin{align*}
Q=[
	(q_6, H_2=(&h_1=[0], h_2=[0], h_7=[0,0], \\
	&h_8=[0,0], h_5=[0], h_6=[0]))
	(q_3, H_1=(&h_1=[0], h_2=[0], h_3=[0], \\
	&h_4=[0], h_5=[0], h_6=[0]))]
	\end{align*}
This is the case after initialization or before any commas are read.

This is the execution of \emph{oneStep}(\emph{NFA}, ``,'', $1$, $Q$):

\begin{enumerate}
\item Initialize \emph{high} as empty stack and fill \emph{low} with hungry threads of all states of $Q$. The stacks are used for
thread scheduling and low priority transitions will be processed only after all 
high priority transitions have had their chance to mark the NFA states.
This is necessary because at most one thread for each NFA state survives 
and is therefore essential that the first thread to arrive at the 
state is the one that has the correct greediness.

We put all threads from $Q$ onto the \emph{low} stack so that every
thread can process all it's results before the next element of $Q$ 
can run. In the execution this means that the greedy threads
can reach their destinations first. $\mathit{low}=[(q_6, H_2), (q_3,H_1))]$.
\item Initialize \emph{buffer} as an empty stack. This needs to 
be done in order to put the most greedy thread into the front position
of our return value. It could well be that the thread is found only after
less greedy threads. \on{What does it mean for a thread to be greedy?}
\item Initialize $R=[]$, the DFA state under construction. We will now proceed to add threads to 
this list in the order in which they need to be run in the next run of \emph{oneStep} in order to obey greediness.
\item The algorithm now begins to push its threads 
forward. Hungry threads that can't consume an transition
starve. If hungry threads consume an edge, they become fed. Fed threads can only follow $\varepsilon$-transitions. 
\item $(q_6,H_2)$ is taken from the \emph{low} stack.
\item $q_6\rightarrow q_7$ can consume ``,''.
\item $(q_7, H_2)$ is pushed to \emph{high}. This means it will be followed right away.
\item $(q_7, H_2)$ is taken from the \emph{high} stack. 
\item It is put on the \emph{buffer} for later processing.
\item Since it it fed, it follows $\varepsilon$-edges.
\item The available transition $q_7\rightarrow q_8$ is evaluated:\begin{enumerate}
	\item 	Before any actions that change the value of a history, the history is copied to avoid
			corruption. The fly-weight pattern allows us to do this in constant time.
	\item	A new history $h$ is created to put the new match for the opening of the group into.
	\item 	$H_2(4) mapsto h$.
	\item 	$h\leftarrow\pos+1$. This is the position after the ``,'', because this is not part of the capture
			group.
	\item	$H_3$ is a copy of $H_2$ with $h$ on position 4. Since $h$ is pointing to the old history, it
			still knows about matches from the past. It can diverge here however.
	\item $(q_8, H_3)$ is put on the \emph{high} stack.
\end{enumerate}
\item $(q_8, H_3)$ is taken from the \emph{high} stack.
\item It is put on the \emph{buffer}. \emph{buffer}$=[(q_8, H_3), (q_7, H_2)]$
\item It can follow no further transitions and dies.
\item $(q_3, H_1)$ is taken from \emph{low}. It is hungry.
\item We now flush our \emph{buffer}: $R=[(q_8, H_3), (q_7, H_2)]$, \emph{buffer}$=[]$.
\item The only transition that consumes ``,'' is $q_3\rightarrow q_4$:\begin{enumerate}
	\item $(q_4, H_1)$ is pushed to $high$ as a fed thread.
\end{enumerate}
\item $(q_4, H_1)$ is popped from $high$.
\item It is pushed to the \emph{buffer}. \emph{buffer}$=(q_4, H_2)$
\item $q_4\rightarrow q_3$ is visited. \begin{enumerate}
	\item $(q_3, H_2))$ is pushed to \emph{low}. This means it will be processed after all high-priority 
	transitions this thread can visit, but it would be visited before any other start threads.
\end{enumerate}
\item We flush the \emph{buffer} again: $R=[(q_8, H_3), (q_7, H_2), (q_4, H_1)]$
\item $(q_3, H_1)$ is taken from the \emph{low} stack.
\item It is added to \emph{buffer}.
\item $q_3\rightarrow q_5$ is visited:\begin{enumerate}
	\item $(q_5, H_1)$ is pushed to \emph{high}.
\end{enumerate}
\item $(q_5, H_1)$ is taken from the \emph{high} stack.
\item It is added to \emph{buffer}.
\item $q_5\rightarrow q_6$ is visited and contains the closing commit of the second capture group:\begin{enumerate}
	\item Two histories are created to store the new positions of both the start and the end of the capture group. This ensures that other threads will not corrupt the memory.
	\item A new history $h$ is created to put the previous match for the opening of the group into.
	\item A new history $h'$ is created for the closing position.
	\item $H_1(2) \mapsto h$. 
	\item $H_1(3) \mapsto h'$.
	\item $h'\leftarrow\pos$. This is the position of the ``,''.
	\item $H_4$ is a copy of $H_1$ with $h$ on position 2 and $h'$ on position 3.
	\item $(q_6, H_4)$ is pushed to \emph{high}.
\end{enumerate}
\item $(q_6, H_4)$ is taken from the \emph{high} stack.
\item It is added to the \emph{buffer}.
\item Both stacks are empty:
\item We flush our \emph{buffer}: 
\begin{align*}
R=[&(q_8, H_3), (q_7, H_2), (q_4, H_1),\\ &(q_6, H_4), (q_5, H_1)]
\end{align*}
\item $R$ is returned.
\end{enumerate}
\end{example}

Our algorithm ensures with algorithm~\ref{findmapping} that the 
compilation of the regular expression terminates by implementing 
the mapping scheme from \cite{Laur00a}. 
Mappings are best understood as an optimization that allows reuse
of DFA states, that are ``similar enough''. DFA states that have the 
same structure can be reused. For example the DFA state 
$Q=\{(q_1, (h_1, h_2))\}$ is structurally no different from 
$Q'=\{(q_1, (h_3, h_4))\}$. Instead of creating a new DFA state $Q'$ 
in the automaton, we can simulate it by a transition to $Q$ with the
added instructions $h_1\leftarrow h_3$ and $h_2 \leftarrow h_4$.

\begin{example} Executing $findMapping$:

After the execution seen in example~\ref{ex:oneStep1} the new proposed state is 
\begin{align*}
Q'=\{(q_4, (&h_1=[0], h_2=[0], h_3=[0], \\
	&h_4=[0], h_5=[0], h_6=[0])), \\
	(q_5, (&h_1=[0], h_2=[0], h_9=[0,0],\\
	& h_{10}=[1,1], h_5=[0], h_6=[0]))\}
\intertext{and the previous DFA state was}
Q=\{(q_3, (&h_1=[0], h_2=[0], h_3=[0],\\
	&h_4=[0], h_5=[0], h_6=[0])),\\
	(q_5, (&h_1=[0], h_2=[0], h_7=[0,0],\\
	&h_8=[0,0], h_5=[0], h_6=[0]))\}
\end{align*}

This is the execution of $findMapping(Q', DFA)$: 
\begin{enumerate}
\item $Q$ is considered as a candidate
\item $q_3$ is checked \begin{enumerate}
	\item All histories can be mapped with the identity.
\end{enumerate}
\item $q_5$ is checked \begin{enumerate}
	\item $h_1, h_2, h_5, h_6$ can be mapped with the identity.
	\item $h_9$ can be mapped to $h_7$.
	\item $h_{10}$ can be mapped to $h_8$.
\end{enumerate}
\item The mapping is successful and the instructions $h_8 \leftarrow h_{10}$ and $h_7 \leftarrow h_9$ are returned.
\end{enumerate}
Note that by construction at most one candidate fulfils the requirement that the same states are present.
\end{example}



\section{Discussion}

\subsection{Compactification}
\seclabel{Implementation}
The most important implementation detail, it brought a factor 10
increase in performance, was the use of a compactified representation
of DFA transition tables whenever possible.  Compactified, here,
means to store the transition table as a struct of arrays, rather
than as an array of structs, as recommended by the Intel optimization
handbook \cite[section 6.5.1]{Inte13a}.  The transition table is a
map from source state and input range to target state and instructions.
Following Intel's recommendation, we store it as an object of five
arrays: \texttt{int[] states; char[] froms;  char[] tos; Instruction[][]
instructions; int[] newStates;}, all of the same length, and ordered
primarily by \texttt{states}, then by \texttt{froms}, which is
lowest character in an input range.  To read a character, the engine
now merely searches in the transition table, using binary search,
for the current state and the current input character.

However, the above structure isn't a great fit with lazy compilation, 
as new transitions might have to be added into the middle of the table
at any time. 
Another problem is that, above, the state is represented as an integer.
However, as described in the algorithm, a DFA state is really a list of NFA states and their 
arrays of histories. If we need to lazily compile another DFA state, all of these need to be examined.

The compromise we found is the following: The canonical representation
of the transition table is a red-black tree of transitions, each
transition containing source and target DFA state (both as the full
list of their NFA states, and histories), an input range, and a
list of instructions. This structure allows for quick inserting of
new DFA states once they are lazily compiled.  At the same time,
lookups in a red-black tree are logarithmic.  Then, whenever we
read a fixed number of input characters without lazily compiling,
we transform the transition table to the struct of arrays described
above, and switch to using it as our new transition table.
If, however, we read a character for which there is no transition, we need to
de-optimize, throw away the compactified representation, 
generate the missing DFA state, and add it to the red-black tree.

The above algorithm chimes well with the observation that usually,
regular expression matching needs only a handful of DFA states, and thus,
compactifying can be done early, and only seldom need to be undone.

\subsection{Intertwining of the pipeline stages}
The lazy compilation of the DFA when matching a string enables us
to avoid compiling states of it that might never be necessary. This
allows us to avoid the full powerset construction\cite{Sips05a}, which has 
time complexity of $O(2^m)$, where $m$ is the size of the NFA.

\section{Benchmark}
\begin{figure}[h]
\includegraphics[width=0.75\linewidth]{graphs/pathological}
\caption{\figlabel{patho} Matching $\textit{a?}^n\textit{a}^n$. Bottom line is our approach, top line is java.util.regex.Pattern.}
\end{figure}

\section{Related work}
Our algorithm is a modification of Laurikari's algorithm \cite{laurikari2000nfas},
which is itself a modified powerset construction algorithm \cite[p. 55]{Sipser2005}.

While there is no shortage of books discussing the usage of regular
expressions, the implementation side of regular expression has not
been so lucky. Cox is spot-on when he argues that innovations have
repeatedly been ignored and later reinvented \cite{Cox07a,Cox09a,Cox10a}. 

This paper is no exception. The authors of this paper had set out
to implement Laurikari's TDFA algorithm \cite{Laur00a},
only to discover that Laurikari's description of a TDFA is so far
from complete that it can rightfully only be called the sketch for
an algorithm. Only late in the process did we discover that the blanks
had already been filled by Kuklewicz in the course of his implementation
of TDFAs in Haskell \cite{Kukl07a}. Kuklewicz enshrined his
added insight into Haskell library, but never published the algorithm
as a whole. If the history of regular expressions is evidence of one
thing, it is that source code is a terrible medium to convey algorithms. 

The situation dramatically improved with Cox's simple and concise
explanation of regular expression matching \cite{Cox07a}. It seems
ironic that this well-versed author published his influential work
on his website. Although the joke may be on Academia's side.

When \del{the taciturn} practioners acknowledge each other's work, we can't
help but disagree almost universally with the characterizations they
produce. Sulzmann and Lu \cite{Sulz12a} call Kuklewicz's
work an ``implementation'' of Laurikari's algorithm, although Laurikari's
algorithm is far too incomplete for that statement to be fair. Laurikari's
algorithm is referred to as a POSIX-style automaton. In truth, Laurikari
leaves the matching strategy entirely open. It was Kuklewicz that
found out how to get POSIX-style matching out of Laurikari's TDFA. 

Cox says that Laurikari's TDFA is a reinvention Pike's published only
in code algorithm \cite{Pike87a}, which is Thompson's NFA with submatch
tracking. This seems unfair in that Laurikari's allows for far more
aggressive \chg{reusing}{reuse} of old states than \del{what} Thompson allows. This should
lead to Laurikari's TDFA having fewer states, and therefore better
performance, than even Google's RE2, which uses Pike's algorithm.
This is not confirmed by the benchmarks by Sulzmann and Lu \cite{Sulz12a},
but they offer an explanation: in their profiling, they see that all
Haskell implementations spend considerable time decoding the input
strings. In other words, the measured performance is more of an artifact
of the programming environment used. 

Another mistake that permeates the scarce literature is to call regular
expression matching linear. As Sedgewick points out correctly \cite{Sedg90a},
Thompson's NFA matching is of complexity $O(mn)$, where $m$ is the
size of the input NFA, and $n$ is the size of the input string. To
call this linear means to assume that $m$ is fixed, which is not justified. It may well be true that, at present,
$m$ tends to be small. But that is a natural consequence of the algorithms
not scaling very well with $m$. If they did, that would allow for
fast feature extracting from text. Therefore, in this paper, we consider
the state of the art algorithms to be quadratic, since both $m$ and
$n$ are part of the input to a regular expression matcher. We cannot
rule out that a linear algorithm exists, in fact, we hope for it.
To insist that regular expression matching is done in linear time
is to insist that the optimal algorithm has already been found; that
is probably not true.

Sulzmann and Lu add to the table a new matching strategy that yields
good practical performance, although the theoretical bounds are considerably
worse than the state of the art, at $O(n^{2}m)$ \cite{Sulz12a}.

\bibliographystyle{plain}
\bibliography{biblio,bib/scg}
\end{document}
